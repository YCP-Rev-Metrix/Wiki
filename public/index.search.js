var relearn_search_index = [
  {
    "content": "Official Project Requirements Document and Slideshow The official project requirements document can be found in the project’s Google Drive here, and the requirements presentation slideshow can be found here\nAbstract RevMetrix is a system that will record metrics of a bowling ball as it travels down the lane. It will consist of a physical device that the user can place inside a bowling ball and mobile application. The device will record rotational and positional data about the ball as it travels down the lane. It will periodically transmit this data to the mobile device to allow the user to understand what was happening to the ball when it was thrown. This will allow the user to improve their skill.\nSimulation The two main portions of the simulation architecture are the Unity Visualizer and the SmartDot simulator. Once complete, the Visualizer will be used to simulate a throw given initial parameters for the throw. The Visualizer will implement realistic physics to ensure that the ball and pins behave exactly as they would in real life. Additionally, simulated data will be collected from the Visualizer and used, alongside data from a binary dump file, to test the SmartDot Simulator. The SmartDot Simulator will simulate the physical device that will be used to get the data. The simulator will sample data from the Visualizer and store, then transmit that data like the SmartDot module would. This data will be transmitted to the mobile device via bluetooth.\nFrontend The frontend section of the diagram is the Mobile Application, Ciclops, the SmartDotAPI, and the File Reader. The Mobile Application serves the designed UI to the end-user of the project. It also transmits all relevant input gathered from the user to the Cloud infrastructure by saving it to a Local Database which is then transmitted to the Cloud. Along with this, the Mobile Application facilitates gathering all user input, whether it is the user inputting a new ball or the user recording a video of them bowling with a new or pre-existing ball. Ciclops will take a video file and use image-processing to extract data from video. The Ciclops program will be able to take the user’s videos and generate positional data for the ball over the throw and then send it to the database. Finally, the SmartDot API will communicate between the mobile app and the SmartDot module within the ball. Currently our plan is to use the API plugins affiliated with the MBIENT Labs Bluetooth IMU to test and further develop bluetooth functionality. We will then work with the backend team to reference their API in constructing our own.\nBackend The backend section of the diagram is made up of the local database, the client side of the Web API, and the cloud-infrastructure which includes both the research and user databases, algorithm unit, cloud controller, and the server side of the Web API. The cloud-infrastructure is accessed through the mobile application by client/server interactions using the Web API. Each interaction is encoded with SSL encryption to ensure the security of any data being sent to or from the server. The local database is stored on the mobile device and stores the user’s most recent practice or match session data.\nCommunication Protocols Secure Socket Layer (SSL) encryption is used to encrypt and decrypt data that is being sent through the Web API in order to ensure the security of that data. Since the data being sent through the Web API will include usernames, passwords, and other possibly valuable data, it must be encrypted to ensure the digital safety of the application’s users. Bluetooth communications will be implemented into the ISmartDot interface in order to receive information from the SmartDot module located inside of the bowling ball. This data will be sent to the SmartDot API and used in the mobile application. SmartDot Module and ISmartDot Interface The SmartDot module is the physical device that will be placed into the bowling ball. It will record positional and rotational data about the ball as it travels down the lane. It will periodically transmit that data to the mobile device for the user to analyze. It will communicate with the mobile device through the ISmartDot interface. The interface will consist of a few commands for reading and writing data to the SmartDot module. In this way, the system will be able to record metrics about the ball and present it to the user.\nAnalysis and Design by Type of Develeopment Backend TeamProject Analysis and Design specific to the backend team\nFrontend TeamProject Analysis and Design specific to the frontend team\nSimulation TeamProject Analysis and Design specific to the simulation team\n",
    "description": "Design and Analysis Information and Links to the Document and Slideshow",
    "tags": null,
    "title": "Analysis and Design",
    "uri": "/Wiki/project-assignments/analysis-and-design/index.html"
  },
  {
    "content": "Progress of Backend Development Backend Architecture Web API - Client and Server The Web API is now capable of making client-server interactions and is being hosted on a Linux Ubuntu 20.04 cloud server through DigitalOcean. To deploy the API application, a plethora of different approaches to the task were conducted, but in the end it was easiest and most efficient to create a Docker image for the application, create a docker-compose file for the application’s Dockerfile, and transfer this file to the DigitalOcean droplet in which the only installations needed on the server were Docker and SQLServer. Though the domain for the API will be changed further into the project, a test page for the API can be visited, but the link is currently being kept private for security reasons.\nBecause of the recent deployment of the API, the frontend team can now make secure data requests and transfers from the cloud databases inside of the mobile application.\nUpdated Database Schemas Below are the most recently updated reworkings of the project’s database schemas. The databases listed below can be seen in the RevMetrix System Architecture\nLocal Database Schema (Localhost) Research Database Schema (Cloud Server) User Database Schema (Cloud Server) Results of database stress testing can be seen at the link below Database Observation DataAll data collected from database engine stress testing\n",
    "description": "All details about the current state of the backend development, along with what is planned for the future",
    "tags": null,
    "title": "Backend - Current State",
    "uri": "/Wiki/current-state-of-revmetrix/backend/index.html"
  },
  {
    "content": "Architecture Initial Database Architecture Local Database (Localhost) Local Database Schema User Database (Cloud Server) User Database Schema Raw Database (Cloud Server) Raw Database Schema ",
    "description": "UML diagrams representing the initial database architecture and schemas",
    "tags": null,
    "title": "Backend - Database Architecture",
    "uri": "/Wiki/initial-project-ideas/initial-database-architecture/index.html"
  },
  {
    "content": " Jordan Carvell, Braden Fleming, Victor Pineda, Robert Wood Backend Architecture Minimal Working System The Backend Team’s role in the project will consist of three main objectives: Creation and maintenance of the Cloud Databases (User and Raw Binary databases) and the local phone database, creation and implementation of the Web API (allowing for client-server interactions), and the development and integration of ISmartDot interface. For the minimal working system, the most important goal to complete will be the decision of which database engine(s) will be used to read and write data between the different technologies used. Once the decision is made, we will need to create the database tables and enable read/write functionality to and from each database’s respective locations. Finally, to complete the minimal working system, we will need to complete the development of the Web API to effectively request and receive data to and from the mobile application and each database.\nIssues Communication between the local and cloud databases may serve as an obstacle due to the possibility of using a different database engine for the local database than the cloud databases use. Hosting the Web Server in a way that allows it to be accessed from anywhere. SSL certificates and network security may get in the way of this.\n50% Working System In the 50% working system, the system components have the ability to communicate with each other. Most of the features for the system will not be implemented, such as a Cloud Controller with basic features; the ability to read the data and format it, or to pass the data to either the Raw and/or User database. What will be functioning is an API client whether it is a dummy or a primary functioning API client that communicates to the basic functioning cloud controller using WebAPI.\nFinal Working System The final working system will have a fully functional system with the ability to communicate throughout the system. The API client will communicate to the Web API that talks to the Cloud controller which can talk to at least one of the databases and give the data back to the cloud controller. The Cloud Controller then transfers data to the Web API which then gives the data to an API Client.\n",
    "description": "Project requirements specific to the backend development",
    "tags": null,
    "title": "Backend Requirements",
    "uri": "/Wiki/project-assignments/project-requirements/backend/index.html"
  },
  {
    "content": " Jordan Carvell, Braden Fleming, Victor Pineda, Robert Wood Local Database The local database will be used to store and retrieve information from the current or most recent practice/match session that the user completed. The mobile app sends the session’s data to the local database and overwrites previous data entries when a new practice/match session is completed and the previous data has been uploaded to the user database in the cloud. This is done initially on the phone app to ensure that data is not lost.\nWeb API - Server The Web API server is the endpoint of the web API that the frontend team can interact with via the client code. This will house controllers and logic to talk to the database, authenticate \u0026 authorize users, and preform algorithmic analysis when necessary.\nWeb API - Client The Web API client is a purpose built C# utility devised to make communications with the Web API server simpler, implementing automatic error handling, request abstraction, POCO conversions, encryption, and authentication. This means that when it comes time to integrate the client into the mobile application, little will need to be done.\nCommon The common project will hold common classes and architecture between the Server, client, and other projects within cloud infrastructure. This will consist of primarily logging-related classes and POCOs.\nAlgorithm Unit The algorithm unit will contain many algorithms developed by Professor Hake during his thesis. These will largely be implemented by other students and exists to determine ball \u0026 throw statistics \u0026 metrics from primarily raw data from the Research Database. This may eventually house algorithms relating to video processing as well.\nUser Database The User database will be storing all of the information from the SmartDot module and Ciclops. It will store all of the User/Game details and then a connection will be established via a Web Api to the phone application that will read and write information to or from the cloud-based database.\nResearch Database The research database is going to hold the raw data that is collected from the Smart Dot Module. This entire database will have a similar layout to the User Database, but instead of being used for displaying purposes it will be utilized for Professor Hake’s analysis. This Database will be used for algorithm development.\nUML Diagrams Web API - Server Web API - Client Common Algorithm Unit A UML diagram has not been devised for this as no algorithm development has taken place.\nDatabase Interface Diagrams User Database Local Database Research Database Database Schemas Local Database User Database Research Database ",
    "description": "Project Analysis and Design specific to the backend team",
    "tags": null,
    "title": "Backend Team",
    "uri": "/Wiki/project-assignments/analysis-and-design/backend/index.html"
  },
  {
    "content": "Contributing documentation is pretty straightforward, follow this basic guide to learn how.\nA brief history This page was built with the Hugo framework \u0026 static site generator using the Relearn theme and hosted on GitHub Pages. Every commit to the main branch will trigger a GitHub Action, causing the page to rebuild itself. This takes only a minute or two before the changes are updated. This structure provides a beautiful site, solid uptime, and the ability to even make edits in the GitHub web editor.\nEditing existing pages Navigate to the page that you would like edit and select the icon in the top right of the page. See Learning the syntax for more information on making edits.\nCreating new pages \u0026 sections Create a new folder, named whatever you want inside of content/\u003cdesired location\u003e. Create a new file here named _index.md, this file will contain your page’s content. If sub-pages are desired, add a new file titled \u003cfilename.md\u003e inside of your newest folder. See Relearn Theme Docs for better documentation.\nLearning the syntax The syntax used here can be as simple as Markdown (Discord uses this), however, may get more advanced if you want something fancy.\nThe following two guides are excellent sources for learning Markdown:\nRelearn Theme Docs Markdown Docs This framework also allows the use of HTML code instead of Markdown, however, Shortcodes may be better suited to what you want to do.\nThe following two references may help you learn the Shortcodes that may be used:\nRelearn Shortcodes Hugo Shortcodes Making local edits In order to make local edits on your own machine, we will need to follow a couple of steps:\nInstall Git if you do not already have it Install Hugo (Extended edition) Install Go Clone https://github.com/YCP-Rev-Metrix/Wiki.git in your favorite IDE or terminal Open a Git terminal in the directory of your new repository. For VS users, this can be (Toolbar) Git \u003e Open in command prompt Launch a live server by calling hugo serve (git or wsl terminal) Navigate a browser to http://localhost:1313/Wiki Make changes (mostly in content directory). After saving, you should see the page refresh live. After making your changes, push or merge your changes back up to the main branch. Your changes will now be reflected on the WWW in a few minutes. ",
    "description": "How to add, edit, and remove documentation on the Wiki",
    "tags": null,
    "title": "Contributing Documentation",
    "uri": "/Wiki/how-to-contribute/contributing-documentation/index.html"
  },
  {
    "content": "Prerequisites Before continuing with any of the following steps, make sure to create an account on DigitalOcean\nCreating a New Project After creating an account, login and head over to the dashboard. From here, in the top left of the page, click the “New Project” button as seen below:\nFill out the necessary fields, and in the “move resources into …” window, click “skip this step” at the bottom to move on since the droplet is being made from scratch.\nCreating a New Droplet To Start After creating a new project, click on the project’s name in the top left of the page under the “Projects” dropdown. This will take you to the project’s dashboard. From here, go ahead and click the “Create” dropdown button in the top menu bar on the project’s dashboard page as seen below:\nFrom here, select the “Droplets” option to continue:\nThis will take you to the specifications portion of creating the droplet, where you’ll select your Region, Datacenter, Image, Size, CPU Options, Additional Storage, and Authentication Method.\nRegion Selecting a region is purely preference, but for optimal performance it is safe to pick the region closest to where the devices are that the applications on the Droplet will be talking to. Selecting a region looks similar to the following image:\nDatacenter Once a region for the Droplet is selected, go ahead and select a datacenter as seen below:\n*Note: It is recommended to choose a datacenter that is close to you or to the application on the Droplet’s users.\nImage Moving forward, select the disk image that you want to use. There are plenty of options, but the standard image to use is a Linux Ubuntu image as they are the most convenient for server-side development. The selection section will look like the following:\nThough the newest version of Ubuntu (23.10 x64) is available on DigitalOcean, it is recommended to use Ubuntu 20.04 x64 as it is the version that currently has long term support (LTS). You can change the version using the dropdown pictured below:\nSize The next specification to choose is the size, which includes the Droplet type and the CPU options. It is simpler to stick with the basic options for now, but you may change the type of droplet and increase memory as needed. The recommended options for a basic Droplet are shown below:\n*Note: The size of the Droplet can be increased after creation, but the size can not be decreased.\nAdditional Storage and Authentication Method After choosing the size of the Droplet, it is time to decide on the addition of extra volumes and the authentication method of the Droplet. Adding additional volumes is optional, however it is recommended in the event that the applications one plans to host on the droplet and their data are expected to need to be relocated later on.\nOn the other hand, it is required to select an authentication method in order to access the Droplet. The authentication method can be chosen based off of preference, but the easiest way to access a Droplet is by setting a password for the “root” user. This way, the console can be accessed through the Droplet’s dashboard and there won’t be a hassle of adding SSH keys to certain machines. Select either option, as seen below:\nRecommended Options Though the following options are recommended, they are not required. However, since the “Add improved metrics monitoring and alerting” option is free and provides the ability to collect and graph expanded system-level metrics, track performance, and set up alerts instantly within the control panel, go ahead and select it as seen here:\nFinalize To complete the creation of the Droplet, go to the “Finalize Details” section (seen below) and select the number of Droplets you wish to create, give each of them a name (either automatically generated or user-specified), and select which project to assign the Droplet(s) to.\nOnce this section is filled out, hit the “Create Droplet” button in the bottom right, as pictured here:\n*Note: You may create the Droplet via Command Line, however, it is easier to do so by the method above.\nConclusion Your Droplet is now created and ready to be used. To view the newly created Droplet, click on the project you’ve assigned it to in the top left of the dashboard page under the “Projects” dropdown, then select the Droplet from the list of them on the project’s dashboard. There, you will find all of the information regarding the new Droplet.\nFor information on how to access the Droplet’s console, view its data usage, and more check out the “Managing a Droplet” page.\n",
    "description": "How to create a DigitalOcean Droplet",
    "tags": null,
    "title": "Creating a Droplet",
    "uri": "/Wiki/how-to-contribute/digitalocean-management/creating-a-droplet/index.html"
  },
  {
    "content": "RevMetrix System Architecture Current Status by Development Team Backend - Current StateAll details about the current state of the backend development, along with what is planned for the future\nFrontend - Current StateAll details about the current state of the frontend development, along with what is planned for the future\nSimulation - Current StateAll details about the current state of simulation development, along with what is planned for the future\n",
    "description": "The current state of each aspect of the project and what is to come",
    "tags": null,
    "title": "Current State of RevMetrix",
    "uri": "/Wiki/current-state-of-revmetrix/index.html"
  },
  {
    "content": "Below is all of the data collected from our database engine stress testing\nMaking 100-million entries in each database engine The Databases orginally tested were PostgreSQL, MySQL, and MongoDB.\nMongoDB was removed from the list of candidates for our database due to the read/write speeds of the others being significantly faster than its own.\nDelete, Update, and Add operations in PostgreSQL \u0026 MySQL PostgreSQL and MySQL tables and performance graphs Due to the large performance difference between PostgreSQL and MySQL, the MySQL database engine was most likely to be removed. However, we continued to test it against the other engines in the event that at higher rates/speeds, it out performed them.\nSQL Server was recommended by Professor Hake, so we looked into it and began testing it against the PostgreSQL and MySQL engines.\nSQL Server tables and performance graphs Adding a single row in each engine Due to the significant differences in speed between SQL Server and PostgreSQL, along with MySQL, we have decided to use SQL Server as our primary database engine for RevMetrix.\n",
    "description": "All data collected from database engine stress testing",
    "tags": null,
    "title": "Database Observation Data",
    "uri": "/Wiki/current-state-of-revmetrix/backend/database-obsevation-data/index.html"
  },
  {
    "content": "Table of Contents Creating a DropletHow to create a DigitalOcean Droplet\nManaging a DropletHow to manage a DigitalOcean droplet\n",
    "description": "Information regarding how to manage DigitalOcean Droplets",
    "tags": null,
    "title": "DigitalOcean Management",
    "uri": "/Wiki/how-to-contribute/digitalocean-management/index.html"
  },
  {
    "content": "Overview Docker is a tool used to encapsulate an application and its development enviornment. Rather than transferring the entire application when deploying it to a server, one can simply create a Dockerfile in their project’s directory and specify what to include in the encapsualted enviornment. After testing the Dockerfile to see if it builds and runs successfully, once can then create a “docker-compose.yml” file, specifying the Docker compose format version (not the actual Docker version), project name, build configuration (including where to pull files from and the location of the Dockerfile previously created), Docker container name, ports to listen on, and Docker image name. Once the docker-compose file builds successfully, the build configurations inside of it can be deleted out, and the docker-compose file can be run locally or transferred to a server machine that has docker installed. After running the docker-compose file, the application should be running, and if it is a web application, the user should be able to visit it at the IP address (of the machine running the docker-compose file) and the port specified in the file.\nPrerequisites Before completing any of the following steps, it is necessary to create a Docker account. After creating an account, download Docker Desktop to easily manage containers and images that are created throughout the process below.\nCreating A Dockerfile For An Application To begin the dockerization of an application, a Dockerfile must be created. Open up the development enviornment of the application and create a file named Dockerfile (no extension) in the root directory of the project. The exact layout of a Dockerfile is specific to every project, but can be modified easily to work with any project. A base Dockerfile for a Python application developed in a Linux Ubuntu 20.04 enviornment would look something like the following:\nFROM ubuntu:22.04 COPY . /app RUN make /app CMD python /app/\u003cApplicationName\u003e.pyThe Dockerfile above creates one layer per instruction:\nFROM creates a layer from the ubuntu:22.04 Docker image. COPY adds all of the files (because of the “.” specified) from the Docker client’s current directory. RUN builds the application with make. CMD specifies what startup command to run within the container. When you run an image and generate a container, you add a new writable layer, also called the container layer, on top of the underlying layers. All changes made to the running container, such as writing new files, modifying existing files, and deleting files, are written to this writable container layer.\nRunning The Application Using The Dockerfile Building And Running The Dockerfile After the Dockerfile for the application is created, navigate to the directory in which it was created and enter the following commands:\ndocker build -t \u003cApplicationName\u003e .After building the application using the Dockerfile, go ahead and run the application with:\ndocker run -p 8000:8000 \u003cApplicationName\u003ewhere the numbers after -p are the ports on which the application should run on. The application should now be running on http://localhost:8000 if that port was originally open. If not, try switching what port the application should run on.\nStopping The Dockerfile To stop the application, simply hit CTRL + C to end the execution.\nCreating The Application’s docker-compose File Now that the project’s Dockerfile is configured, built, and successfully running, it is time to create the docker-compose file. In the same location as the Dockerfile created above, create a new file named docker-compose.yml.\nA docker-compose file for the application above would look similar to the following:\nversion: '3' services: \u003cApplicationName\u003e: build: context: . dockerfile: DockerfileIn the docker-compose file above, multiple things take place:\nThe version of the Docker compose format is specified as version 3. A service by the name or the application (indicated by “ApplicationName”) is defined to represent the application. The build context is specified as the current directory where the Dockerfile is located using the context key (\".\") The dockerfile key is set to “Dockerfile”, the name of the Dockerfile to use when building the application. Building The Application After adding the above to the docker-compose file, open a new terminal session and enter the following command to build the application:\ndocker-compose buildCreating A Repository For The Image Once the application is built using the above command, go to DockerHub in a web browser and login to a Docker account. From the dashboard, go to the Repositories tab in the top menu, then hit the “Create Repository” button on the page. To create a new repository for the application, the following fields must be filled out, though the description is optional:\nNamespace - Usually the username of the account, should autofill immediately and can be left as such Repository Name - Can be anything, but it is good practice to name it the same as the application being dockerized Short Description (optional) - Could be anything or left blank Visibility (Public or Private) - Select Public if the image should be available to other Docker users to pull and run locally, Private otherwise Editing The Docker Compose File After an image is created in DockerHub, change the contents of the docker-compose file to the following, either deleting or commenting out the build configuration:\nversion: '3' services: \u003cApplicationName\u003e: image: \u003cNamespace\u003e/\u003cRepositoryName\u003e ports: - 8000:8000In the reconstructed docker-compose file, the build is removed in order to run the application using the file. This time, the file specifies the image location (the one created in DockerHub above) by the Namespace and RepositoryName set in the previous step, along with specifying which desired ports the application should run on.\nSpinning Up The Docker Container Now, go ahead and run the following command to run the docker-compose file:\ndocker-compose upThis command will spin up a Docker container for the application’s image and begin running it on port 8000 of the machine that it is running on.\nStopping The Docker Container After testing the application as much as desired, stop the application with the following command:\ndocker-compose downAfter stopping the application, open the Docker Desktop application and if not already logged in, do so. From the dashboard, navigate to the Images tab on the left side of the screen, and find the image created earlier in DockerHub in the Image menu. To the right of the image’s name (“Namespace/RepositoryName”), there is an Actions tab. Under the Actions tab, click “Show image actions” (three vertical dots), then go ahead and click the “Push to Hub” button. Now, the image created by the docker-compose file is available under the repository created on DockerHub earlier.\nDeploying The Docker Container On Another Machine If the application is meant to be hosted on a different machine, the “docker-compose.yml” file can simply be transferred to the desired machine using any file transfer protocol. However, since only the file needs to be transferred and not the entire application, it is easier just to create another file named “docker-compose.yml” on the desired machine and copy the contents of the most recent version to the new one.\n*Note: Docker must be installed on the destination machine to run the Docker compose file. Depending on what operating system is being used to host the application, DockerDocs has instructions on how to install Docker depending on the user’s OS.\nOnce the Docker engine is installed on the destination machine and the Docker compose file has been created, the file can be run the same way as before:\ndocker-compose upThe above command runs the application but only until the terminal session is closed. Once closed, the Docker runtime is killed off and the application is no longer running. To avoid this, in the event that the goal is to run the application 24/7 (i.e. for a web application), run the Docker compose file in detached mode using -d the same as below:\ndocker-compose up -dIf the container was spun up successfully, the application will continue running after the terminal session is closed.\nTroubleshooting and Informational Commands Below you can find a list of most of the useful troubleshooting and informational commands regarding Docker containers:\nContainer Status To check the status of all Docker containers on a machine, enter the following command:\ndocker statusCheck Which Containers Are Running To check what containers are currently spun up on the machine, use the following command to get a list of them in the terminal:\ndocker psPrinting Container Logs The docker ps command will print out the ID, Image, Command, Created, Status, Ports, and Names fields of each container running.\nIf one wants to check the logs of a specific docker container currently running on the machine, it is easist to run the command above to see the container ID of the specific container that one wants the logs for. Rather than typing in the entire ID into the following command, one only needs to enter the first character of the ID of the container that they wish to view the logs for. For a container with the ID “5a2b3c4d5e6f”, one can just enter the first character, “5”, into the following command:\ndocker logs -f container-idSo after determining the ID of the container one wishes to view the logs for, say container with the ID “5a2b3c4d5e6f”, the command to be run would be:\ndocker logs -f 5",
    "description": "Information on how to dockerize a project and deploy onto a separate machine or server",
    "tags": null,
    "title": "Dockerizing an Application",
    "uri": "/Wiki/how-to-contribute/dockerizing-an-application/index.html"
  },
  {
    "content": "Progress of Frontend Development Frontend Architecture Frontend User Interface Wireframes Local Testing Database Schema ",
    "description": "All details about the current state of the frontend development, along with what is planned for the future",
    "tags": null,
    "title": "Frontend - Current State",
    "uri": "/Wiki/current-state-of-revmetrix/frontend/index.html"
  },
  {
    "content": " Theo Bloomquist, Thomas Bywaters, Michael Hensel, John Kettula, Liz Mains Frontend Wireframes Minimal Working System Frontend Team’s role is to create a mobile app that is able to facilitate communication between what is developed by both the Graphics and Backend teams. The minimal working system will consist of an interface that allows for user input and displays user-specific data in response. This will also include the framework for communicating with the Graphics and Backend teams, to be fully implemented later on, this also includes Bluetooth functionality. There will also be camera and camera roll access to later be used to analyze a bowler’s throw.\nIssues Communication between all the moving pieces is a crucial part of the application, however, transmission of graphical data can be incredibly difficult due to its size, which may result in increased latency affecting the user of the mobile application. Along with this, figuring out the specifics of Bluetooth integration will be specific to the output of the chip installed into the bowling ball, meaning it will have to be developed around the hardware-implemented into the chip. Connecting to a device requires specific information to be known, which may be difficult to gather from a simple prototype chip. Bluetooth integration for cross-platform devices is going to be another challenge because Android and iOS have different permission systems.\n50% Working System Implementation of a new user-input interface that allows the user to manage (through creation, deletion, and editing) a list of bowling balls that each have data respective to their real-life values as well as associated Bluetooth IDs. Along with this, enhancements should be made to the communication framework between the mobile application and what is developed by the Backend and Graphics teams to decrease latency and improve end-user experience. Along with this the phone application will have newly implemented functionality that allows for a video to be pulled in from an end-user’s device, this will serve as a proof of concept for further image processing, which will take user video and convert it into positional data.\nIssues Potential issues that we could run into with this milestone are the creation/deletion/editing of a list of bowling balls. The main challenge will be the organization of bowling balls by the user and implementing the local database into the app. This is because the process of adding a bowling ball into the database will span across two communication processes. The first process will be the interaction between the Bluetooth scanner and the app. The second process that will interact with the database is the Maui framework. These two processes combined could cause some issues with the phone app, just in terms of communication.\nFinal Working System For the final working system, we will expand upon the prototype communication to give the app the capability to communicate with the backend servers and implement the Unity visualizers. The app will also be able to display graphical information based on information pulled from the database. To effectively launch the application in a mobile environment, several algorithmic and storage modifications and enhancements will be made to ensure a smooth smartphone experience. A local database will be worked into this design to allow for user data such as usernames, passwords, and bowling balls to be saved locally until the complete server database is established. The groundwork for image processing similar to Dr. Babcock’s Ciclops will also be started for future teams to expand upon. To create a user-friendly app, even in the early production stages, several UI improvements will be made to shift the application away from a testing environment into a user-focused experience.\nIssues Some potential issues that we might run into include the final data size of all of the features we plan to implement. Displaying graphical information and storing graphical information will take up local space. To ensure optimal functionality, the amount of data stored in the mobile app must be minimized. Another issue will be the conversion of the GUI from the Desktop to the Phone. Different features might not make it to both the Phone app and Desktop app. Implementing Ciclops into our app will be another issue in terms of the size of the application. Ciclops is, essentially, another application that will need to work within our application, increasing the app size further. Image processing within Ciclops will be another challenge because it will require the discovery of different imaging algorithms and the discovery of a new framework. More specifically, differentiating between the bowling ball and the background will be a challenge because of all of the different parts of a bowling lane. There’s a potential problem where the bowling ball will blend with other objects.\n",
    "description": "Project requirements specific to the frontend development",
    "tags": null,
    "title": "Frontend Requirements",
    "uri": "/Wiki/project-assignments/project-requirements/frontend/index.html"
  },
  {
    "content": " Theo Bloomquist, Thomas Bywaters, Michael Hensel, John Kettula, Liz Mains The frontend will be handling the user interface in the form of a Mobile Application. Data is pulled into the application from the SmartDot API within the bowling ball as well as the Ciclops Program which videos the throw itself. The data from both the SmartDot and Ciclops are processed and pushed into the Local Database. The Local Database will then send the data out to the server. The Unity Visualizer is embedded into the application and is fed data from a specific throw by the Mobile Application.\nThe mobile application gives the user the ability to interact with and utilize the features of RevMetrix. Upon opening, the user is presented with a login screen where they can either sign in to an existing account or create a new one. If the user creates an account, an instance of the User class is created and their chosen username and password is set and stored in the database. If the User already has an account, the entered username and password is verified with those stored in the database, the User class is populated with their data, and they are granted access to the home page. From the homepage, the user has several options. They can view saved bowling balls or add new ones, record a throw using the SmartDot in a saved ball, record a throw video using the Ciclops camera, or view previous throws as video or SmartDot data.\nSmartDot API The SmartDot API will facilitate communication between the mobile app and the SmartDot inside the user’s bowling ball. Using Bluetooth LE, the user will connect to a SmartDot and, after signaling the SmartDot to start, may begin recording their throw. Users will have to first scan for and select a SmartDot using bluetooth. Upon first connection, a new ball will be created for them as an instance of the Ball class. Saved bowling balls will remember their SmartDot’s bluetooth ID to allow for instant connection. While the ball is in motion during the throw, the SmartDot will transmit data back to the mobile app that will store it in an instance of the Throw class to be referenced later. Recording of data stops when the ball reaches the end of the lane or when the user signals so from the app.\nFile Reader The File Reader will allow the user to import video files into the desktop application, so that it can then be analyzed by Ciclops, which will extract bowling statistics from the video. A potential application of the file reader could also be writing local files to the local device. The File Reader could also be manipulated to generate files based on the data received from the SmartDot API. The File Reader is implemented within the application and is able to transmit data across the application using local variables \u0026 the local database.\nCiclops An implementation of Ciclops will be another feature in the app that will analyze a bowler’s throw and extract data from it. It will use image recognition algorithms to differentiate between the bowling ball and the bowling lane. It will use each individual frame and compare it to the next frame to get varying statistics about the ball. These statistics include: speed, velocity, and curve. This information will be incredibly useful for extrapolation to calculate the metrics that will be eventually served to the user. Ciclops will be implemented directly in the application and will store information directly onto the applications local database.\nMobile Application User Interface All Frontend UI will be implemented using XAML backed with C#, allowing for users to view outputted, processed data that is displayed onto the screen using dynamically allocated XAML text Label classes, as well as allowing for users to input textual information \u0026 click buttons. Button Classes come standard with XAML applications, their specific function is to handle “Clicked” input from the user, when “Clicked” is triggered by the user clicking a button a function defined in the XAML.CS class is executed. This level of abstraction allows for buttons to have dynamic functionality, meaning that whatever functionality is provided by one button to the user could be entirely different from the functionality provided by another button to the user. Textual input displayed on the XAML page can be handled in one of two ways, “TextChanged” \u0026 “Completed”. “TextChanged” is triggered when any text within the input field is changed and, runs similarly to “Clicked” as it, it runs a function that is defined with the XAML.CS class. “Completed” functions similarly to “TextChanged,” triggering a specified event when the Enter key (Tab on Windows) is clicked. Dynamic output that is viewed by the user is handled by variables defined within the XAML.CS classes; these variables are passed from the XAML.CS class to the XAML output field (usually a text Label class) through the use of Binding Paths. Binding Paths are used by XAML classes to reference data, such as variables, that are defined and allocated by XAML.CS classes, Binding Paths access the current value of this data and are then displayed to the user by the Label class. Navigation between pages is handled by both the PushAsync and GoToAsync function that takes the String name of a pre-existing XAML \u0026 XAML.CS class that defines the structure of a page that will be pushed onto the stack to be displayed to the user.\nClass Methods SmartDot API ScanAsync() ConnectToDeviceAsync() ConnectToKnownDeviceAsync(Guid) ReadAsync() ReadServicesAsync() ReadCharacteristicsAsync() ReadDescriptorsAsync() DisconnectAsync() File Reader MakeFile(string fileName) WriteFile(string fileName, string text) Create(string fileName) Ciclops TakePhoto() OnTakeVidBtnClicked(object sender, EventArgs e) Mobile Application User Interface Clicked(object sender, EventArgs e) TextChanged(object sender, TextChangedEventArgs e) Completed(object sender, EventArgs e) PushAsync(String page) GoToAsync(String page) User Login(string, string) CreateAcct(string, string) AddBall(IDevice) NewGame() DeleteBall(Ball) DeleteGame(Game) Game AddThrow() DeleteThrow(Throw) UML Diagram MVC Architecture Due to the design architecture of .NET MAUI, both the XAML \u0026 XAML.CS classes can be considered the “controller” classes as they both handle data flow to each other. However the specific definitions of the UI are clearly handled by the XAML class and the model for which outputted data is structured is handled by the XAML.CS class. While this may be difficult to grasp at first, there are sections of code within both the XAML and XAML.CS classes that handle \u0026 execute control logic amongst themselves, making these sections both the respective ‘Controller’ of this diagram.\nLocal Testing Database Schema The primary purpose of the local database that is hosted by the frontend application is to temporarily store user information so that it can be transmitted via Web API to the backends Cloud infrastructure for long-term storage. The backend team will be handling the creation of this local database, while we will create a dummy database purely for testing purposes. This is to allow for the frontend team to retrieve data to ensure the functionality of application features. As seen above, the schema of this database is almost identical to the schema of the backend team’s local database as they will be storing similar information. The main difference being the User Table which will be used for testing users while the server databases are being built. This dummy database is also to ensure seamless integration between the frontend and backend team when they eventually come together to combine their systems.\n",
    "description": "Project Analysis and Design specific to the frontend team",
    "tags": null,
    "title": "Frontend Team",
    "uri": "/Wiki/project-assignments/analysis-and-design/frontend/index.html"
  },
  {
    "content": "",
    "description": "How to create a branch of the cloud-infrastructure repository and make edits",
    "tags": null,
    "title": "GitHub Workflow - Backend",
    "uri": "/Wiki/how-to-contribute/github-workflows/backend-workflow/index.html"
  },
  {
    "content": "Prerequisites Most of these instructions at JetBrains Rider specific, as that is what we are using to develop the Frontend Application.\nCreating an Issue Go to the Frontend Github repository, using the following link: https://github.com/YCP-Rev-Metrix/FrontEnd/tree/master\nWithin this page click on the Issues tab, which should be found in the following location: After your screen has refreshed, you should see a screen similar to the following screen below. After this has been done select New Issue: Give the issue a name and a description, and make sure to click Assign Yourself under the Assignees tab. After this is done, assign the issue a Label that is corresponding to whatever you are trying to implement (whether it’s patching a bug or adding new functionality) and click Submit New Issue. This issue will eventually be connected with a branch but this can’t happen until that branch is created so onto our next topic:\nCreating a Branch Run the following commands within a terminal window:\ngit checkout -b {name of branch} \u003c- creates branch locally\nEx:\ngit checkout -b 8-update-gitignore\ngit push origin {branch name}\nThis branch will now show up on the Github repo on the website GUI, to ensure this branch is up to date with the master run the following commands within the same terminal:\ngit fetch\ngit merge origin/master\nOnce this is done be sure to associate the branch to an issue\nLinking an issue to a branch Within the Issues tab click on the issue you previously created and click the gear next to the Development section on the right side. After clicking this gear, traverse through the Github repository and select your newly created branch. This will allow you to close this issue once this branch has been PR’d into the master, as the branch should also then be deleted to create a new branch to tackle the next issue. -Basically a branch is a temporary work space rather than a permanent one -Create an issue-\u003emake a branch for it-\u003eCode a bunch-\u003ePR-\u003edelete-\u003erepeat\nCreating a Pull Request Select the Commit section found in the top left section of the Rider tab, shown below: Select the files you would like to Commit (Found in Unversioned Files) and check their boxes. DO NOT commit .gitignore. Once they’ve been selected press Commit and Push… (If you selected Commit you must follow the instructions specified in {Section B} before trying to make a PR if you selected Commit and Push… feel free to ignore them)\nAfter the changes have been Pushed to the remote branch, go to the Github repository website and you should see something like this: If you select Compare \u0026 pull request Github will automatically generate you a Pull Request that will pull your branch into master.\nOnce the pull request has been accepted and merged into master, delete your branch \u0026 issue as specified below.\n{Section B} If you selected “Commit” you must follow these instructions before attempting to make a PR Click on the Git button found in the bottom left section of the tab, shown below: Once that has been opened, right-click on the Local branch that you’ve been working on (in this example it’s 18-Screentoscreen underneath the Local section) and select Push…\nOnce your Local branch has been pushed to the Remote, the Remote branch will have all the changes that you have coded on your device and a Pull Request can be made.\nDeleting a Branch Input these commands into your terminal:\ngit branch -d {branchname} \u003c- deletes branch locally\ngit push origin --delete {branchname} \u003c- deletes branch remotely\n",
    "description": "How to create a branch of the frontend repository and make edits",
    "tags": null,
    "title": "GitHub Workflow - Frontend",
    "uri": "/Wiki/how-to-contribute/github-workflows/frontend-workflow/index.html"
  },
  {
    "content": "Making General Changes Swap the current Github branch to the Test branch: Click on the “Fetch Origin” button for the Test branch: Proceed to make the necessary changes for the Github Issue you are working on Review your changes in Github Desktop: Swap to your personal branch and bring the new changes over Include Github Issue # in the commit subject (i.e. Issue #12, new pin model) Commit your changes Go to the Github repository’s website and hit the New Pull Request button Make sure that your source and destination are both correct before creating the pull request This is (test \u003c– yourUsername) almost always Create pull request, then copy the link into the PR and send to Discord Make sure to tag the @Graphics Dev Team Approving Git Pull Requests Either check the current pull request on Github.com or click the link in Discord Open the Files Changed tab to review changes made Review changes, and if everything checks out, approve and submit review ",
    "description": "How to create a branch of the simulation repository Test branch and make edits",
    "tags": null,
    "title": "GitHub Workflow - Simulation",
    "uri": "/Wiki/how-to-contribute/github-workflows/simulation-workflow/index.html"
  },
  {
    "content": "Table of Contents GitHub Workflow - BackendHow to create a branch of the cloud-infrastructure repository and make edits\nGitHub Workflow - FrontendHow to create a branch of the frontend repository and make edits\nGitHub Workflow - SimulationHow to create a branch of the simulation repository Test branch and make edits\n",
    "description": "All GitHub workflows for the project",
    "tags": null,
    "title": "GitHub Workflows",
    "uri": "/Wiki/how-to-contribute/github-workflows/index.html"
  },
  {
    "content": "Table of Contents Contributing DocumentationHow to add, edit, and remove documentation on the Wiki\nDigitalOcean ManagementInformation regarding how to manage DigitalOcean Droplets\nDockerizing an ApplicationInformation on how to dockerize a project and deploy onto a separate machine or server\nGitHub WorkflowsAll GitHub workflows for the project\nRevMetrix WebAPIInformation on how to use the WebAPI\n",
    "description": "",
    "tags": null,
    "title": "How To Contribute",
    "uri": "/Wiki/how-to-contribute/index.html"
  },
  {
    "content": "Original System Architecture Backend - Database ArchitectureUML diagrams representing the initial database architecture and schemas\nPhone App - Proof of ConceptDrawing board ideas for the layout and interaction methods of the phone application\nSimulation - First ThoughtsThe base ideas around using Unity for the development of a bowling simulation\n",
    "description": "",
    "tags": null,
    "title": "initial Project Ideas",
    "uri": "/Wiki/initial-project-ideas/index.html"
  },
  {
    "content": "The ISmartDot interface will allow a layer of abstraction between the Application and the SmartDot or SmartDotSimulator.\nThe sole function of this interface is to allow reading and writing from pages of memory stored on the SmartDot, and make / maintain connections to the SmartDot hardware.\nISmartDot interface /** * Interface representing the basic functionality of the Bluetooth connection between the mobile * application and a SmartDot implementation (Simulator \u0026 Hardware) */ public interface ISmartDot { /** * Summary: Attempts to create a connection to the SmartDot hardware * Params: connectionParameters * Returns: Version representing the SmartDot hardware version (APIs use), null * if unable to connect */ public Task\u003cVersion?\u003e Connect(ConnectionParameters connectionParameters); /** * Summary: Tests if the connection to the SmartDot hardware is present * Params: connectionParameters * Returns: Boolean representing if there is a connection to SmartDot hardware */ public Task\u003cbool\u003e IsConnected(); /** * Summary: Asynchronous method to write one page to the SmartDot implementation * Params: address -\u003e The page start addess in memory * buffer -\u003e Data (byte[]) to write to the SmartDot implementations page * bytes -\u003e Length of data to write from * Returns: A bool representing if call was successful */ public Task\u003cSmartDotTransactionResult\u003e WriteBytes(long address, byte[] buffer, long bytes); /** * Summary: Asynchronous method to read one page from the SmartDot implementation * Params: address -\u003e The page start addess in memory * buffer -\u003e Ref data (byte[]) read from the SmartDot implementations page (will attempt to fill completely) * bytes -\u003e Length of data to read into * Returns: A bool representing if call was successful */ public Task\u003cSmartDotTransactionResult\u003e ReadBytes(long address, ref byte[] buffer, long bytes); /** * Summary: Sends a command to the SmartDot hardware * Params: command -\u003e The string command to send to the SmartDot hardware * Returns: Result from the command execution on the SmartDot hardware */ public Task\u003cstring\u003e OutgoingSmartDotCommand(string command); /** * Summary: Invokes IncomingSmartDotCommandCallback method. Should be executed only from the * implementor. * Params: command -\u003e String command called from SmartDot hardware * Returns: n/a */ protected virtual void RaiseIncomingSmartDotCommand(string command) =\u003e IncomingSmartDotCommandCallback?.Invoke(command); /** * Summary: Incoming command callback (from SmartDot hardware). Signals from the SmartDot hardware * that something important happened. * Params: none * Returns: n/a */ public Action\u003cstring\u003e? IncomingSmartDotCommandCallback { get; set; } }SmartDotTransactionResult enumeration /** * Enum representing the transaction result state of a communication with SmartDot hardware */ public enum SmartDotTransactionResult : byte { /** * SmartDot transaction succeeded */ SUCCESS, /** * SmartDot transaction failed - unknown cause */ FAILURE_UNKNOWN, /** * SmartDot transaction failed - data corruption */ FAILURE_PERMISSION_ERROR, /** * SmartDot transaction failed - device disconnected */ FAILURE_DISCONNECTED, /** * SmartDot transaction failed - data corruption */ FAILURE_DATA_CORRUPTION }ConnectionParameters class /** * Parameters necessary to successfully connect to the correct SmartDot hardware */ public class ConnectionParameters { }The SmartDotAPI will be responsible for interpreting data sent and received from an ISmartDot interface implementor by accessing the version # on the first memory page.\n",
    "description": "Information about the ISmartDot interface",
    "tags": null,
    "title": "ISmartDot Interface",
    "uri": "/Wiki/smartdot/ismartdot/index.html"
  },
  {
    "content": "Prerequisites Before continuing with any of the following steps, make sure that you have created an account on DigitalOcean and have already created a Droplet. If you have not yet created a Droplet, see the “Creating a Droplet” page to find out how to do so.\nNavigating to the Droplet To begin learning how to manage a DigitalOcean Droplet, select the “Projects” dropdown in the top left of the main dashboard page and click on the project in which the Droplet you want to manage is located:\nYou will then be taken to the project’s dashboard, where you can see resources and activity and manage the project’s settings. Click on the resources tab and select the Droplet from the list below:\n*Note: The full name of the project, Droplet, and the full IP address of the Droplet shown are blurred for security purposes.\nDroplet Dashboard After navigating to the Droplet, you will be greated by the Droplet’s dashboard, as seen below:\nFrom here, you can view and manage a plethora of different things regarding the Droplet, including:\nTabs Details Graphs Graphical data that shows bandwidth, CPU usage, and disk I/O operations Access Access to the Droplet console, Recovery console, and reset the root password Power Turn off the Droplet or Power Cycle the Droplet Volumes Add or manage disk volumes on the Droplet Resize Upgrade the Droplet Type and CPU Options of the Droplet Networking View details of the Public and Private Networks and manage Inbound and Outbound Firewalls Backups Create backups of the Droplet (priced per month by size of Droplet) Snapshots Take a snapshot and manage previous snapshots taken of the Droplet Kernel View the status of the Kernel or enable management of it in the control panel History View all previous changes made to the current Droplet Destroy Destroy or Rebuild the current Droplet (only used to get rid of the Droplet completely) Tags Manage tags assigned to the Droplet (tags are used for grouping Droplets and are not required) Recovery Select whether to Boot from ISO or Boot from Harddrive (used if corruption in Droplet occurs) Seeing as only a handful of these tabs are necessary to use, we’ll only cover the basics throughout the rest of the steps. If more information is desired on the other tabs, finish the rest of the steps and play around inside of the Droplet’s dashboard to gain a better understanding of just what you can do to/with the Droplet from within each tab.\n*Note: All IP addresses associated with this Droplet have been crossed out for security purposes.\nAccess The access tab is the most important out of the list, as it allows the user to log into the Droplet console and Recovery console, along with giving them the ability to change the password of the “root” user. Below you will find information regarding each of the access tab’s functions.\nDroplet Console The Droplet console is what the user will use to access their Droplet, make changes, and perform any modifications/operations inside of the Droplet. Pictured below is the form for signing into the console:\nHere, the default login user is “root”, and the password for the user will be entered once the console popup window shows after clicking the “Launch Droplet Console” button. The password to enter is the same one that the user set for “root” at the time of creation of the Droplet.\nAfter logging into the console as root, the user will have full permissions to add files, edit files, run programs, setup a file transferring protocol, etc. We won’t go into the specifics since what a user does inside of their console is out of the bounds of simple Droplet management.\nRecovery Console Next is the Recovery console. In the event that one loses access to their Droplet or has lost data that they are unable to recover in the regular Droplet console, they can open a Recovery console session to attempt to recover access or potentially lost data. To access the Recovery console, simply click the “Launch Recovery Console” button on the access tab page and enter the password for the “root” user in the popup window. The button for the Recovery console looks something like below:\nAfter logging into the Recovery console, the user can select from 6 options available. These options are explained in detail when the user logs in, so there is no need to elaborate on them here.\n*Note: The Recovery console is only to be used if familiar with Ubuntu and navigating through an OS using a terminal.\nReset Root Password The “Reset Root Password” button can be used to reset the password in the event that the user has forgotten their password or has lost access to their Droplet. To do so, simply hit the button shown below and DigitalOcean will prompt accordingly.\nPower The power tab can be used to power off the Droplet completely or to Power Cycle the Droplet. Below more information can be found regarding both functions:\nPower Off The power off function can be used to shutdown the Droplet in the event that maintenence needs to be done on it or the user wants to enable IPv6 addressing for the Droplet. More details on the power off function can be seen in the following image:\nPower Cycle The power cycle function can be used to do a full restart of the Droplet, powering it off then turning it back on. More details on this can be found in the image below:\nNetworking The networking tab is where all options regarding IP addressing and firewalls are located. Below you can find more information regarding each option listed on the page:\nPublic Network The public network settings show the public IP addresses and subnets that people can access the Droplet through. In the same section, users can enable or disable public IPv6 addressing if so desired. See the below screenshot:\n*Note: All IP addresses associated with this Droplet have been crossed out for security purposes.\nPrivate Network The private network settings list the Droplet’s private IP address and the name and IP of the the Virtual Private Cloud (VPC) network. As shown below, only users that are a part of the same VPC network can connect to the Droplet using the private IP address:\n*Note: All IP addresses associated with this Droplet have been crossed out for security purposes.\nFirewalls Below you will find information regarding the firewall settings for the Droplet.\nThere are two types of firewall rules that can be configured. How one configures their firewalls depends solely on the goal of the user. Below you will find the inbound and outbound rules and what type of connections each firewall handles:\nInbound Firewall:\nThe inbound firewall handles incoming connections by a multitude of protocols such as HTTPS, SSH, and so on. To specify a port that an application on the Droplet should run on, one would simply create a “Custom” inbound firewall rule that specifies the port of the application to run. Outbound Firewall:\nThe outbound firewall handles outgoing traffic from the server different communication protocols such as ICMP, TCP, UDP, and more. These rules allow the user to specify what ports and what destinations traffic from the server is allowed to be sent to. Snapshots The snapshots tab is where users are able to create snapshot backups of their Droplet. Though it costs a bit extra to create snapshots, it is good practice to create one in the event that data gets corrupted on the Droplet or the user loses access to it and can not reset their password. Below you can see the details of creating a snapshot and that all you need to do is name the snapshot and hit “Take Live Snapshot”:\nUsers can also see previous snapshots taken in the bottom section of the page and if none were made, the output above for that section will be shown.\n*Note: The name of the Droplet snapshot has been crossed out for security purposes.\nDestroy The destroy tab can be used to permanently delete the Droplet or rebuild the droplet from a previously taken snapshot (after doing so in the above section). See the below image to get an idea of each function:\nRecovery The recovery tab is used when someone needs to restore something from the Droplet that they may have accidentally deleted or that has become corrupted. The default setting in this tab is to boot from the Droplet’s hard drive, which is what is happening each time the Droplet is turned on until the option is changed. Changing the option to boot from recovery ISO allows the user to handle different kernel mismatches and attempt to restore any corrupted data, though none of the recovery methods are guaranteed to work in the case of an emergency (deleted privilege folders, renaming of privilege folders, etc.). See below to read more about each option’s function:\n",
    "description": "How to manage a DigitalOcean droplet",
    "tags": null,
    "title": "Managing a Droplet",
    "uri": "/Wiki/how-to-contribute/digitalocean-management/managing-a-droplet/index.html"
  },
  {
    "content": "Milestone 1 Main Task The task is to demonstrate a minimal working version of your system. For a “minimal” working system: Your minimal working system should implement the most important classes in the core object model, and test them comprehensively using unit tests What are the most important requirements of your system? You should be able to demonstrate to us that they are at least partially implemented, with appropriate navigation between the components of the system. Extremely important: whatever functionality you demonstrate should be implemented in your core object model classes. We do not want to see a hacked-together “prototype” that looks good but internally is spaghetti code. Also, do NOT demonstrate your system components in isolation - those components should be interacting with each other, at least on a basic level. Because this milestone involves implementing several of your most important use cases or user stories, you will need to make sure that your use cases/user stories are documented. Make sure that your project’s issue tracker has an issue for each use case, and each use case issue has a reasonably detailed textual description of the use case. You can write your use cases as full-blown use cases (see Chapter 9 of UML Distilled). The important concern is that you document the important functionality of the system from the perspective of the user. You will not receive full credit for implementing use cases unless they are documented. The Presentation / Demonstration Each project team should plan to present for about 45 to 60 minutes. Larger teams (with multiple sub-teams) will likely go longer than that. There will likely also be an extensive question and answer period. All presentations need to be limited to less than two hours (including the question and answer period).\nYou should use some presentation software such as PowerPoint or Google Slides. Your slides should contain brief bulleted points and graphics (tables, diagrams, screen captures, wire frames) that provide overall context for your bullet points. Slides should not be a “wall of text”, and please avoid reading your slides or from a prepared script.\nYour presentation should include the following elements:\nProvide a brief description of the project. Provide a high-level overview of your system architecture. Explain what tools/technologies you are using, and what role they play in your project development. Explain what parts of your design are implemented, and which parts remain to be implemented by referencing your updated UML diagram(s) and Database Schema. Demonstrate the system in action. Walk through your UI (and/or internal workflow) and explain the Use Case functionality it demonstrates. Discuss your automated test strategy, and run your unit tests to demonstrate your testing framework, and describe what is happening. Briefly talk about how you plan to evolve the system to implement the remaining functionality. Please do a dry run before class. Because of the limited amount of time we have in class, we can’t wait for your team to troubleshoot issues that arise during the presentation. Milestone 1 - Presentation / Demonstration Links The Google Slides presentation for milestone 1 can be found here.\nThe Team ",
    "description": "Progress made on the minimal working system and the associated slideshow",
    "tags": null,
    "title": "MS1 - Minimal Working System",
    "uri": "/Wiki/project-milestones/ms1-minimal-working-system/index.html"
  },
  {
    "content": "Main Task The task is to demonstrate a 50% working version of your system. For a 50% working system: You should have implemented the majority of the pages and navigation between those pages for your UI. You should present the major Use Cases by giving a demo of the navigation. This does not mean that every page is fully functional, but rather that the majority of the pages exist, and that button and other navigation functionality works. You should be able to use multiple pages to enter, modify, delete information. And, those changes should be persistent, meaning that they are reflected in your database. You should be able to show that the various components of your system interact with each other. For example, if you have a system that involves a website, a mobile app (or access via a mobile device), hardware and/or HW simulator, and a backend server/DB, show that you have a functional end-to-end connection between all of those components. You should have a significant number of test cases that cover your controller scenarios (not just getter and setter tests). If you have established a CI/CD (continuous integration/continuous development) server, show that it works. The Presentation / Demonstration Each project team should plan to present for about 45 to 60 minutes. Larger teams (with multiple sub-teams) will likely go longer than that. There will likely also be an extensive question and answer period. All presentations need to be limited to less than two hours (including the question and answer period). You should use some presentation software such as PowerPoint or Google Slides. Your slides should contain brief bulleted points and graphics (tables, diagrams, screen captures, wire frames) that provide overall context for your bullet points. Slides should not be a “wall of text”, and please avoid reading your slides or from a prepared script. Your presentation should include the following elements: Provide a brief description of the project. Provide an updated high-level overview of your system architecture. Explain what tools/technologies you are using, what role they play in your project development, and any changes you may have made since Milestone 1. Explain what parts of your design are implemented, and which parts remain to be implemented by referencing your updated UML diagram(s) and Database Schema. Walk through your UI (and/or internal workflow) and explain the use case functionality it demonstrates. Discuss your automated test strategy, and run your unit tests to demonstrate your testing framework, and describe what is happening. Briefly talk about how you plan to evolve the system to implement the remaining functionality. Plan your demonstration carefully. You need to convince us that your system is 50% complete. Make sure your demonstration convinces us! Please do a dry run before class. Because of the limited amount of time we have in class, we can’t wait for your team to troubleshoot issues that arise during the presentation. Milestone 2 - Presentation / Demonstration ",
    "description": "Progress made on the 50% working system and the associated slideshow",
    "tags": null,
    "title": "MS2 - 50% Working System",
    "uri": "/Wiki/project-milestones/ms2-50-working-system/index.html"
  },
  {
    "content": "Main Task The task is to complete a final report documenting your project, and to give a presentation on the project. Our expectations for the report are described in the Final Report Details document. We strongly encourage you to make an appointment and visit the Writing Center in the Center for Teaching and Learning to get help from a writing tutor. Bring your document with you! Presentation Guidelines Each project team will give a presentation of about 60 to 75 minutes. Larger teams (with multiple sub-teams) will likely run longer. There may also be an extensive question and answer period. All presentations need to be limited to less than two hours (including the question and answer period). You should use some presentation software such as PowerPoint or Google Slides for your presentation. Your slides should contain brief bulleted points and graphics that provide overall context for your bullet points. Slides should not be a “wall of text”, and please avoid reading your slides or a prepared script. NOTE: You will have already submitted your draft technical report before your final milestone presentation. You can leverage the draft report contents for the figures, diagrams, pictures, screen captures, etc that will appear in your presentation. Please rehearse your presentation! We can’t allow any presentation to go over its allotted time. You may wish to have a few notecards for reference. However, you should not read directly from your notecards or from your slides. This makes for a very boring and painful presentation. Keep in mind that the audience for your presentation consists of students and faculty in CS and Engineering, as well as your clients, alumni and technical advisors. Don’t assume that the audience knows anything about your specific project. However, you may assume that the audience is reasonably knowledgeable about computers, software, etc. Don’t get too bogged down in details. As with any form of technical communication, you want to emphasize the most important and interesting information. Provide supporting details if they are necessary, but otherwise try to keep the presentation at a fairly high level. Here is a suggested structure for your presentation: Background: What problem were you trying to solve? Discuss the state of the project at the begining of the Fall, and the new/modified system requirements that you addressed. Show a diagram of your overall system architecture, how the pieces fit together and interact, and how data flows between them. Analysis and design: Discuss your design model. In this part of the presentation, you must show a UML class diagram illustrating the most important classes and methods in your system, and how they relate/interact with each other. Please use multiple UML diagrams if a single diagram would have too many classes. You must also show your Database Schema, and show how your tables interact with your classes. You may also use multiple slides for your datbase schema for clarity. This is also a good opportunity to discuss how the design of the system changed as you worked on the implementation. Discuss the development tools/technologies that you ended up using. Describe what they contribute to the project. Implementation: What were the most interesting things you encountered/learned when you implemented the system? If you used any interesting programming techniques, this is a good opportunity to discuss them. Do not show code during your presentation. Demonstration: Show your system working. Demonstrate the most important/interesting features. You should have a scripted demonstration that you practiced multiple times prior to the presentation. One person runs the demonstration while others describe what is happening, and why. You do NOT need to show every feature of your software, but certainly show the most important/interesting/challenging aspects of your project. If you have a multiple major components to your system (frontend UI, mobile app, HW, simulator, backend server/DB), demonstrate end-to-end scenarios where information from one end reaches all the eay to other end, and possibly back again. Discuss your automated test framework, and demonstrate it in action. Future Work: Since most teams are working on multi-semester/multi-year projects, describe the state of the project as it will be for the incoming team in the Spring. Also, show that the future work you present is captured as issues in your issue tracker. Many of you may be resuming work on the same project in the Spring, and there could be new students joining the project. Conclusions: Sum up what you accomplished and what you learned, particularly for underclassmen. If there are aspects of the project you would do differently if you started again from scratch, mention them. You can also talk about how you might want to extend the system in the future. All of the members of your team must make significant contributions to the presentation. Milestone 3 - Document and Presentation / Demonstration ",
    "description": "Progress made on the final working system and the associated slideshow",
    "tags": null,
    "title": "MS3 - Final Working System",
    "uri": "/Wiki/project-milestones/ms3-final-working-system/index.html"
  },
  {
    "content": ".NET MAUI documentation: -Bluetooth Connection\n-NET MAUI General Documentation\n-NET MAUI File Picker\n-NET MAUI Youtube Channel\n-Xamarin Youtube Channel\n-Xamarin Data Binding Documentation\n-Xamarin Advanced Data Binding Documentation\nInitial Proof of Concept - Mobile Application ",
    "description": "Drawing board ideas for the layout and interaction methods of the phone application",
    "tags": null,
    "title": "Phone App - Proof of Concept",
    "uri": "/Wiki/initial-project-ideas/phone-app-proof-of-concept/index.html"
  },
  {
    "content": "Overview Below you can find the list of project assignments completed throughout the fall 2023 semester and all of their details.\nList of Assignments Analysis and DesignDesign and Analysis Information and Links to the Document and Slideshow\nProject RequirementsDetails of the project requirements and the associated slideshow\n",
    "description": "A list of all assignments completed throughout the fall 2023 semester",
    "tags": null,
    "title": "Project Assignments",
    "uri": "/Wiki/project-assignments/index.html"
  },
  {
    "content": "Overview Below you can find the assignment descriptions for each of the project’s milestones. There are three in total, and once the assignment date has passed, the appropriate slideshow can be seen under each milestone’s title.\nMilestones MS1 - Minimal Working SystemProgress made on the minimal working system and the associated slideshow\nMS2 - 50% Working SystemProgress made on the 50% working system and the associated slideshow\nMS3 - Final Working SystemProgress made on the final working system and the associated slideshow\n",
    "description": "The progress made prior to each project milestone",
    "tags": null,
    "title": "Project Milestones",
    "uri": "/Wiki/project-milestones/index.html"
  },
  {
    "content": "Official Project Requirements Document and Slideshow The official project requirements document can be found in the project’s Google Drive here, and the requirements presentation slideshow can be found here\nSystem Architecture Initial Requirements by Type of Develeopment Backend RequirementsProject requirements specific to the backend development\nFrontend RequirementsProject requirements specific to the frontend development\nSimulation RequirementsProject requirements specific to the simulation development\n",
    "description": "Details of the project requirements and the associated slideshow",
    "tags": null,
    "title": "Project Requirements",
    "uri": "/Wiki/project-assignments/project-requirements/index.html"
  },
  {
    "content": " Table of Contents Overview Configuration Database Interactions Adding Endpoints Overview The WebAPI can be complicated from the outside, but after a bit of work, it’s actually not too confusing. There are serveral portions that will be interesting to us, all of which located in Server project.\nConfiguration appsettings.json This file contains general settings for running in different environemnts and with different configurations. Take a look below at the example. Adding to this is farily simple. Config.cs opens the file at runtime and sets relevant variables such as AuthAudience.\n{ \"Logging\": { \"LogLevel\": { \"Default\": \"Information\", \"Microsoft.AspNetCore\": \"Warning\" } }, \"ApiSettings\": { \"Auth\": { \"Audience\": \"RevMetrix\", \"Issuer\": \"https://localhost:7238/\", \"SecretLength\": 32 } } }Program.cs Through this, the authentication (JWT), Middleware, Swagger, and endpoints are all configured. This will run every time and cannot be modified as much based on the enironment or run settings. There is a small area for if the environemnt is development. Below is a snippet taken from Program.cs.\n_ = builder.Services.AddSwaggerGen(); _ = builder.Services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme) .AddJwtBearer(options =\u003e { options.TokenValidationParameters = new TokenValidationParameters { ValidateIssuer = true, ValidateAudience = true, ValidateIssuerSigningKey = true, ValidateLifetime = true, ValidIssuer = Config.AuthIssuer, ValidAudience = Config.AuthAudience, IssuerSigningKey = ServerState.SecurityHandler.AuthorizationSigningTokenKey, ClockSkew = TimeSpan.FromMinutes(5) }; }); WebApplication app = builder.Build(); // Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) { _ = app.UseSwagger(); _ = app.UseSwaggerUI(); } // app.UseHttpsRedirection(); // Verify token not blacklisted _ = app.UseMiddleware\u003cVerifyJWTBlacklistMiddleware\u003e();Environment variables Environment variables in our case are set by external programs. Take a look at below at AbstractDatabase from the DatabaseCore project. Here, the environment variable ‘DOCKERIZED’ is checked, if it has been set, then the DB connection string will change. Inside of the docker config, this environment variable is set, allowing the DatabaseCore to know that it is inside of docker, so it may connect to the DB correctly.\npublic AbstractDatabase(string databaseName) { // Get DOCKERIZED environment variable string? DockerizedEnviron = Environment.GetEnvironmentVariable(\"DOCKERIZED\"); if (DockerizedEnviron == \"Dockerized\") { // Running in docker ConnectionString = $\"Server=sql_server;database={databaseName};User Id=SA;Password=BigPass@Word!;TrustServerCertificate=True;\"; } else { // Likely running locally ConnectionString = $\"Data Source=localhost;database={databaseName};Integrated Security=True;TrustServerCertificate=True;\"; } LogWriter.LogInfo($\"DB Connection: {ConnectionString}\"); DatabaseName = databaseName; Initialize(); } Database Interactions Adding Endpoints ",
    "description": "Information on how to use the WebAPI",
    "tags": null,
    "title": "RevMetrix WebAPI",
    "uri": "/Wiki/how-to-contribute/webapi/index.html"
  },
  {
    "content": "Security is a crucial part of the Web API and security is managed in multiple ways. Please ensure that the security of the application remains during development.\nHttps Authentication and Authorization JWT Refresh Tokens Roles Middleware Secure Randomization Security Methods Hashing Salting Signing Encryption Https Our server forces the use of HTTPS for security purposes, to ensure data sent between the API and Client is encrypted. Specifically, we use TLS, the modern version of SSL.\nLearn more here\nAuthentication and Authorization Authentication and Authorization are the two primary ways of limiting a resource to specific users.\nAuthentication is the process of validating a user’s identity through credentials such as username and password.\nAuthorization is the process of limiting specific resources through roles or permissions. ie. only admins may change a user’s name.\nJWT JWT is an acronym standing for Json Web Token and it is used to provide both Authentication and Authorization to users. This token is granted to the user upon login, set-up by the following in Program.cs:\n_ = builder.Services.AddAuthentication(JwtBearerDefaults.AuthenticationScheme) .AddJwtBearer(options =\u003e { options.TokenValidationParameters = new TokenValidationParameters { ValidateIssuer = true, ValidateAudience = true, ValidateIssuerSigningKey = true, ValidateLifetime = true, ValidIssuer = Config.AuthIssuer, ValidAudience = Config.AuthAudience, IssuerSigningKey = ServerState.SecurityHandler.AuthorizationSigningTokenKey, ClockSkew = TimeSpan.FromMinutes(5) }; });The above specifies that JWTs on our server use the following:\nIssuer validation Audience validation Expiration validation (JWTs expire after a specified time) Issuer (us) Audience (us) Signing key (from server state) Clock skew (in case the clocks are a few minutes off) The big thing here is the expiration and signing. When a JWT is created by the server, it is assigned an expiration time, normally just a few hours in the future, at which point the JWT will no longer be validated. This new JWT is then signed by the server so that the server can detetect if a someone has tampered with the token as all data is in plaintext.\nThe server will also add to the JWT, the user’s roles, username, and a random id.\nView a real server JWT here\n\"header\" { \"alg\": \"HS256\", \"typ\": \"JWT\" } \"payload\" { \"sub\": \"Test User\", \"jti\": \"352bbdc1-0645-4a0f-b2cb-44c5d93fad7f\", \"iss\": \"https://localhost:7238/\", \"aud\": \"RevMetrix\", \"role\": \"user\", \"nbf\": 1698882420, \"exp\": 1698886020, \"iat\": 1698882420 }Learn more here\nRefresh Tokens Refresh tokens are used by the client to request a new JWT from the server when their current one expires. The server, when authorizing the user, will provide back a JWT and Refresh token. In the future, this Refresh token can be use to re-authorize the user. These tokens are just random bytes though could have taken the form of a JWT too: II7IWJueHQjdaxN28MfG7yGuR5KonICYVuexvFIYjiY=\nCurrently, these are stored in the DB with an expiration data and assosiated user.\nLearn more here\nRoles Roles are used to manage user’s abilities and controls and should be handled as security items. Endpoints or controllers that require specific roles will have the following attribute [Authorize(Roles = \"Admin, Manager\")]. This specific example allows only users with either the role of ‘User’ or ‘Manager’ to access this item. Items without the Roles specification will allow a user with any role to access it.\nThese roles are stored in the User DB in the user table as an array\nMiddleware Middleware is a method by which the server may interrupt a client’s communication to an API endpoint and manipulate data or block the request. This has been used to ensure that a user’s JWT has not been blacklisted prior to access of a protected endpoint. Relevant code is shown below:\npublic class VerifyJWTBlacklistMiddleware : IMiddleware { public async Task InvokeAsync(HttpContext context, RequestDelegate next) { Endpoint? endpoint = context.GetEndpoint(); if (endpoint != null) { if (endpoint.Metadata.GetMetadata\u003cAuthorizeAttribute\u003e() != null) { string? token = context.Request.Headers[\"Authorization\"].FirstOrDefault()?.Split(\" \").Last(); if (!string.IsNullOrEmpty(token)) { if (!ServerState.TokenStore.IsAuthorizationBlacklisted(token)) { await next.Invoke(context); return; } } context.Response.StatusCode = 401; return; } } await next.Invoke(context); } }The middleware must also be registered with the application in Program.cs\n// Verify token not blacklisted _ = app.UseMiddleware\u003cVerifyJWTBlacklistMiddleware\u003e();Secure Randomization Not all random functions are created equal. Take Python for example, if you wanted to generate secure randomness, you should not use the Random library as it is not built for cryptology and is susceptible to timing attacks and seed cracking. This applies to C# too, so we use the System.Security.Cryptography module, which provides the RandomNumberGenerator class which is secure.\nA SecurityHandler class has been created to abstract security items in a way that makes sense to more developers, allowing them to avoid worrying about security as much. This class has secure methods to generate random byte arrays, tokens, and other simple methods.\nAn instance of this class may be used from the static ServerState.\nSecurity Methods Passwords remain one of the primary methods to ensure a specific user’s security and users may not always change their password for all the services they use. This means that it is the job of our applicaiton, to ensure that these passwords are stored and used safely. There are many things that go into modern security measures. Here, we will cover the four main topics used in our application.\nHashing Hashing is the method by which a password is turned from plaintext to a format that cannot be reversed. This ensures that if the DB is leaked to the public or if someone has access to it, the passwords themselves remain secure. The WebAPI currently uses SHA256 (for simplicity - this should be changed to something meant for passwords) to generate password hashes however this may easily be change. Hashing will always generate the same output given the same input\nWe can convert our password from a string into hex so it may be hashed: abc123 -\u003e 0x616263313233\nWe can then run SHA256 on this: SHA256(0x616263313233) -\u003e 0x6ca13d52ca70c883e0f0bb101e425a89e8624de51db2d2392593af6a84118090\nThese hashed values can then be stored in the DB. When a user needs to be authenticated through the WebAPI, they provide their password, it is hashed, then compared with the hashed real password in the DB\nWe can convert an incorrect password from a string into hex so it may be hashed: incorrectPassword -\u003e 0x696E636F727265637450617373776F7264\nUsing the incorrect password: SHA256(696E636F727265637450617373776F7264) -\u003e 0x11e56efd5eae13673eccfed917ac1d69f02a8d0b8a4c384c5cc18c1bd2b7dc29 != 0x6ca13d52ca70c883e0f0bb101e425a89e8624de51db2d2392593af6a84118090\nUsing the correct password: SHA256(0x616263313233) -\u003e 0x6ca13d52ca70c883e0f0bb101e425a89e8624de51db2d2392593af6a84118090\nOnline ASCII to hex converter\nOnline SHA256 generator\nExternal hash explanation\nSalting Salting is the method of adding randomness to hashed passwords, so that the same password, when hashed on multiple sites or by different users will not generate the same hash. This means that if a user re-uses passwords, a DB leak will not give this information away as the password hash will appear different. This also means that if two users have the same password, a DB leak will not give this information away as the password hash will appear different.\nA salt is just a random hex value such as: 0x9a66190e4f\nWe can turn the password from a string into hex: abc123 -\u003e 0x616263313233\nLet’s append the salt to the end of the password hex: 0x616263313233 + 0x9a66190e4f -\u003e 0x616263313233|9a66190e4f\nLet’s hash this now: SHA256(0x616263313233|9a66190e4f) -\u003e 0x6509f3df9e7fa53ef15ae4dbc76717097982c59f0e7571bfcd5c864377432cab\nWhen we store this salted and hashed password, we can store the salt that we used too in plaintext in the DB, then in the future, when we need to authorize with a provided password, we can salt that with the same salt that was used for the real password in the DB.\nExternal salt explanation\nRandom salt / hex generator\nSigning Signing is the process of generating a unique value that represents a plaintext value combined with the server’s random cryptographic key.\nLearn more here\nEncryption Encryption is the process of securing text in a way that only the specified parties can read it.\nLearn more here\n",
    "description": "Security information regarding the WebAPI",
    "tags": null,
    "title": "Security",
    "uri": "/Wiki/how-to-contribute/webapi/security/index.html"
  },
  {
    "content": "Progress of Simulation Development Simulation Architecture Demo There should have been a video here but your browser does not seem\rto support it.\rSimulation Dataflow SmartDot Simulator Illustration Textured Lane and Gutters Setup Lane Reset Visual Pinsetter Development Preview Links SmartDot Simulator ",
    "description": "All details about the current state of simulation development, along with what is planned for the future",
    "tags": null,
    "title": "Simulation - Current State",
    "uri": "/Wiki/current-state-of-revmetrix/simulation/index.html"
  },
  {
    "content": "Beginning Ideas of Simulation Design: Initial Lane Setup Full Lane View Lane While Testing Translation Scaled Pins Setup ",
    "description": "The base ideas around using Unity for the development of a bowling simulation",
    "tags": null,
    "title": "Simulation - First Thoughts",
    "uri": "/Wiki/initial-project-ideas/simulation-first-thoughts/index.html"
  },
  {
    "content": " Luke Dodson, Bryce Neptune, Enoch Sam, Ian Viveiros SmartDot Dataflow Diagram Minimal Working System The minimal working system will consist of a basic physics simulator and a SmartDot simulator that can run on pre-recorded data. The SmartDot simulator will imitate the actual hardware but will receive data from data files instead of sensing the environment. It will send this data to the mobile device via Bluetooth. The physics simulator will maintain realistic physics (friction, momentum, rotational acceleration, etc.), but in a simple environment. The lane will act as a flat object with a uniform coefficient of friction.\nIssues The biggest challenge for making the minimal working system will be properly imitating the SmartDot module. The SmartDot simulator will have to have sensors, memory, process synchronization, and data transfer protocols. Simulating all of that in C# will be quite difficult. Additionally, another issue will be verifying that the simulated physics works as it does in real life.\n50% Working System Once the minimal working system is complete. The next step will be to extend the functionality of the SmartDot simulator. The fifty percent milestone will feature the ability to generate new data. The physics simulator will allow users to enter initial parameters to determine how the ball is thrown. The SmartDot simulator will then have sensors that can actually collect the data from the throw and store it. This will allow the simulator to generate new data.\nIssues The most serious challenge for this step will be developing accurate sensors in Unity. The sensors will have to detect light and acceleration and output it in proper binary format. Getting the information might be difficult as there may not be support for it in the Unity framework. There will have to be research into that later.\nFinal Working System The final working system for this semester will have two additional features. The first will be the visualizer. The visualizer will record videos of each throw and save it to the local machine. These videos will then be used by the front end when testing their image processing software. The second feature will be a more realistic environment for the simulator. That will include lighting patterns and lane textures. The lighting patterns will imitate the lighting patterns at bowling alleys. This will help generate more accurate data from the light sensors. In addition, the lane will also have small variations in the surface and oil patterns that will affect how the ball rolls down the lane. This will help increase the accuracy of the accelerometer data.\nIssues Setting the groundwork for the Visualizer will be the physics simulation, the videos produced by this simulation have to be compatible with everything developed for image processing. So having the digital visualization match the same parameters as real-life footage is predicted to be a delicate and tedious task. It is expected that the camera angle may be adjusted multiple times during the same playback, so that multiple angles may be captured of the same shot. This will allow Image Processing to be developed in a more robust, and consistent manner.\n",
    "description": "Project requirements specific to the simulation development",
    "tags": null,
    "title": "Simulation Requirements",
    "uri": "/Wiki/project-assignments/project-requirements/simulation/index.html"
  },
  {
    "content": " Luke Dodson, Bryce Neptune, Enoch Sam, Ian Viveiros SmartDot Simulation The SmartDot Module will be a small device that can be placed in the finger insert of a bowling ball. It will record rotational and positional data about each ball throw and transmit this data via bluetooth to the mobile device. The SmartDot will have two sensors: an 3-axis accelerometer and an ambient light sensor. The light sensor will be used to detect start up conditions by sensing light-to-dark and dark-to-light transitions. It will also help record rotational information by sensing when the ball is pointing up (light) and when the ball is pointing down (dark). The accelerometer will sense the translational acceleration (positional data) and the axis tilt (rotational data). The SmartDot will also have memory to store at least twenty-five samples of each sensor which is equal to twenty-five throws. The SmartDot will also have a bluetooth transmitter that will communicate with the mobile device through the ISmartDot interface. The ISmartDot interface will have several methods. It will have a couple methods for the mobile device to connect with the SmartDot module. It will also have read and write methods for sending and receiving data from the SmartDot. And finally, it will have methods for sending commands to the SmartDot. These commands will be strings. This will make the system more modular. These methods will define the communication between the SmartDot module and the mobile device.\nISmartDot Interface /// \u003csummary\u003e /// Interface representing the basic functionality of the Bluetooth connection between the mobile application and a SmartDot implementation (Simulator \u0026 Hardware) /// \u003c/summary\u003e public interface ISmartDot { /// \u003csummary\u003e /// Attempts to create a connection to the SmartDot hardware /// \u003c/summary\u003e /// \u003cparam name=\"connectionParameters\"\u003eConnection parameters\u003c/param\u003e /// \u003creturns\u003eversion representing the SmartDot hardware version (APIs use), null if unable to connect\u003c/returns\u003e public Task\u003cVersion?\u003e Connect(ConnectionParameters connectionParameters); /// \u003csummary\u003e /// Tests if the connection to the SmartDot hardware is present /// \u003c/summary\u003e /// \u003creturns\u003eBoolean representing if there is a connection to SmartDot hardware\u003c/returns\u003e public Task\u003cbool\u003e IsConnected(); /// \u003csummary\u003e /// Asynchronous method to write one page to the SmartDot implementation /// \u003c/summary\u003e /// \u003cparam name=\"address\"\u003eThe page start address in memory\u003c/param\u003e /// \u003cparam name=\"buffer\"\u003eData (byte[]) to write to the SmartDot implementations page\u003c/param\u003e /// \u003cparam name=\"bytes\"\u003eLength of data to write from \u003cparamref name=\"buffer\"/\u003e\u003c/param\u003e /// \u003creturns\u003eA bool representing if call was successful\u003c/returns\u003e public Task\u003cSmartDotTransactionResult\u003e WriteBytes(long address, byte[] buffer, long bytes); /// \u003csummary\u003e /// Asynchronous method to read one page from the SmartDot implementation /// \u003c/summary\u003e /// \u003cparam name=\"address\"\u003eThe page start address in memory\u003c/param\u003e /// \u003cparam name=\"buffer\"\u003eRef data (byte[]) read from the SmartDot implementations page (will attempt to fill completely)\u003c/param\u003e /// \u003cparam name=\"bytes\"\u003eLength of data to read into \u003cparamref name=\"buffer\"/\u003e\u003c/param\u003e /// \u003creturns\u003eA bool representing if call was successful\u003c/returns\u003e public Task\u003cSmartDotTransactionResult\u003e ReadBytes(long address, ref byte[] buffer, long bytes); /// \u003csummary\u003e /// Sends a command to the SmartDot hardware /// \u003c/summary\u003e /// \u003cparam name=\"command\"\u003eThe string command to send to the SmartDot hardware\u003c/param\u003e /// \u003creturns\u003eResult from the command execution on the SmartDot hardware\u003c/returns\u003e public Task\u003cstring\u003e OutgoingSmartDotCommand(string command); /// \u003csummary\u003e /// Invokes \u003csee cref=\"IncomingSmartDotCommandCallback\"/\u003e. Should be executed only from the implementor. /// \u003c/summary\u003e /// \u003cparam name=\"command\"\u003eString command called from SmartDot hardware\u003c/param\u003e protected virtual void RaiseIncomingSmartDotCommand(string command) =\u003e IncomingSmartDotCommandCallback?.Invoke(command); /// \u003csummary\u003e /// Incoming command callback (from SmartDot hardware). Signals from the SmartDot hardware that something important happened. /// \u003c/summary\u003e public Action\u003cstring\u003e? IncomingSmartDotCommandCallback { get; set; } }/// \u003csummary\u003e /// Enum representing the transaction result state of a communication with SmartDot hardware /// \u003c/summary\u003e public enum SmartDotTransactionResult : byte { /// \u003csummary\u003e /// SmartDot transaction succeeded /// \u003c/summary\u003e SUCCESS, /// \u003csummary\u003e /// SmartDot transaction failed - unknown cause /// \u003c/summary\u003e FAILURE_UNKNOWN, /// \u003csummary\u003e /// SmartDot transaction failed - data corruption /// \u003c/summary\u003e FAILURE_PERMISSION_ERROR, /// \u003csummary\u003e /// SmartDot transaction failed - device disconnected /// \u003c/summary\u003e FAILURE_DISCONNECTED, /// \u003csummary\u003e /// SmartDot transaction failed - data corruption /// \u003c/summary\u003e FAILURE_DATA_CORRUPTION }Unity Visualizer The Unity Visualizer is a Unity simulation that contains a lane, a ball, gutters, and pins. The Visualizer will utilize built-in Unity physics as well as realistic implemented physics in order to make the simulation as accurate as possible to an actual throw. The SmartDot Simulator will be used inside the Visualizer to get new data. Once the Visualizer is complete, it will be used to generate video files and simulated data in order to test other components of the project.\nUnity Camera The Unity Camera is a virtual camera that will exist within the Unity Visualizer. This camera will be set up to record throws within the Visualizer and export the recording as a video file. The video can then be read by the file reader on the mobile device and analyzed by the Ciclops application. Having a simulated video will also allow the Frontend Team to test finding a ball path prior to using actual video footage of real throws.\nUnity Simulated Data The Unity Simulated Data will be data that is gathered from the Unity Visualizer. When the simulation is run, data about the ball’s position, velocity, acceleration, angular velocity, and light level will be recorded and sent to the SmartDot Simulator for testing.\nClass Methods Unity Camera startRecord() endRecord() getVideo() Unity Visualizer setLoft(float h) setInitialVelocity(Vector3 velocity) setCurrentVelocity(Vector3 velocity) setCurrentAngularVelocity(Vector3 angle) setThrowPos(float x, float angle) setBallType(int id) setPinsLeft(int[10] pins) setOilPattern(float[][] pattern) getPinsLeft() getThrowNumber() getThrowRotations() getThrowCurve() getThrowVelocity() exportSimVideo() exportSimBinaryData() start() rollBall() pauseVisualizer() Unity Simulated Data No methods Existing Binary Dump File No methods SmartDot Memory read(int page) write(int page, int byte) SmartDot Bluetooth Transmitter readPage(int page) readPage(int ball_record_page) writePage(int page, int byte, string data) setDefault() Ambient Light Sensor sample() get_cur_time() start_up() check_valid_activation() 3-Axis Accelerometer sample_x() sample_y() sample_z() get_cur_time() Real Time Clock get_cur_time() ",
    "description": "Project Analysis and Design specific to the simulation team",
    "tags": null,
    "title": "Simulation Team",
    "uri": "/Wiki/project-assignments/analysis-and-design/simulation/index.html"
  },
  {
    "content": "\rISmartDot InterfaceInformation about the ISmartDot interface\nSmartDot HardwareInformation about the SmartDot hardware\nSmartDot SimulatorInformation about the SmartDot Simulator\n",
    "description": "",
    "tags": null,
    "title": "SmartDot",
    "uri": "/Wiki/smartdot/index.html"
  },
  {
    "content": "",
    "description": "Information about the SmartDot hardware",
    "tags": null,
    "title": "SmartDot Hardware",
    "uri": "/Wiki/smartdot/smartdothardware/index.html"
  },
  {
    "content": "SmartDot Architecture ",
    "description": "Information about the SmartDot Simulator",
    "tags": null,
    "title": "SmartDot Simulator",
    "uri": "/Wiki/smartdot/smartdot-simulator/index.html"
  },
  {
    "content": "Stuff about NuGet package (github actions)\nInclude autogenerated swagger (github actions)\nShould hugo be re-run after cloud infrastructure actions?\nSomething Something Something\n",
    "description": "Information on how to use the WebAPI",
    "tags": null,
    "title": "Web API Client",
    "uri": "/Wiki/how-to-contribute/webapi/client/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/Wiki/categories/index.html"
  },
  {
    "content": "Faculty Contributors Professor Donald Hake II Dr. David Babcock Student Contributors Simulation / Graphics Team\nLuke Dodson Bryce Neptune Enoch Sam Ian Viveiros Frontend Development Team\nTheodore Bloomquist Thomas Bywaters Michael Hensel John Kettula Elizabeth Mains Backend Developement Team\nJordan Carvell Braden Fleming Victor Pineda Robert Wood ",
    "description": "",
    "tags": null,
    "title": "Credits",
    "uri": "/Wiki/more/credits/index.html"
  },
  {
    "content": "Welcome to RevMetrix Quick Links Current state of RevMetrix How to contribute Initial Project Ideas Project Assignments Project Milestones SmartDot ",
    "description": "",
    "tags": null,
    "title": "RevMetrix",
    "uri": "/Wiki/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/Wiki/tags/index.html"
  }
]
